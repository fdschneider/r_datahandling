---
title: "Data handling in R"
author: "Florian D. Schneider"
date: "27 Februar 2018"
output: 
  html_document: 
    keep_md: yes
---


# Introduction

This workshop covers basic and advanced data handling in R. 

We will talk about how to

- prepare raw data for R (using Excel or Libre office)
- read data into R and write file output from R
- access columns and rows
- review your data (getting summaries etc.)
- fix errors in your data, deal with missing data etc.
- add columns or covariates

and more advanced: 

- convert data structure using tidy and dplyr
- prepare data for publication 


## Preparations

The commands and examples are tested for R > v3.0! Please run the follwing command to ensure that you have all required packages installed: 

```{r, eval = FALSE}
install.packages(c("getPass", "RCurl", "XML", "dplyr", "tidyr", "ggplot2"))
```

```{r load packages, message=FALSE, warning=, include=FALSE}
require("getPass"); #require("RCurl"); require("XML")
require("dplyr"); require("tidyr"); require("ggplot2")
```

Also, a suite of online scripts and data will be required. Please run this block to make them available: 

```{r message=FALSE, warning=FALSE}
source("https://gist.githubusercontent.com/fdschneider/9f5d044d5091976741ad07dab5439a77/raw/faed4eca5ed142797e3c9431029d02c8d46d39dc/accessBexis.R")

```


This lesson assumes basic knowledge in R, like

- how to do calculations
- how to assign objects and differences of object types (vector, data.frame)
- basic understanding of functions (how to apply a function, apply parameters)
- how to get help about a function (`?` and manpages)
- how to read data tables into R 
- how to pick a subset of rows and columns from your table

If you don't feel comfortable with these topics, please walk through this basic script before the workshop: 
http://fdschneider.de/r_basics/ 


```{r, echo = FALSE}
d1 <- read.csv("d1.csv")
d2 <- read.csv("d2.csv")
```

## Online reference


**The presentation script and exercise can be found [here]()!**

Please report difficulties on my Github page: . 

<a rel="license" href="http://creativecommons.org/licenses/by/4.0/"><img alt="Creative Commons License" style="border-width:0" src="https://i.creativecommons.org/l/by/4.0/88x31.png" /></a><br /><span xmlns:dct="http://purl.org/dc/terms/" href="http://purl.org/dc/dcmitype/Text" property="dct:title" rel="dct:type">R data handling script</span> by <span xmlns:cc="http://creativecommons.org/ns#" property="cc:attributionName">Florian D. Schneider</span> is licensed under a <a rel="license" href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution 4.0 International License</a>.


---



# 1. prepare your data for R: raw-data structure 

Usually, the dataset you want to use was assessed in the experimental environment in the field, or in a laboratory. Either via direct input or via a paper notebook, the data ended up in some spreadsheet software, mostly MS Excel or Libre Office Calc. 

## data structure

Of course, R will be able to read in data tables of any structure. Any data structure can be transferred with a couple of lines of code into any form you require. But you can simplify your life if you decide on a **raw data structure** that is easy to handle in R. To achieve this, there are a couple of rules of thumb: 

- **one line = one data point**:  Long-table formats are more tidy than wide-tables (Whickham 2014), e.g. if you keep replicated measures in own columns it is much more work to tell R why they are basically the same, but different. Rather add another column to label treatments or replicates. 
- **keep measured raw data seperate from transformed or derived data**: Raw data are sacred! Maintain a static version of the original raw  data and use R to create derived metrics (e.g. means of replicates) or estimates to fill gaps. 
- **save in .csv or .txt format**, not .xlsx; mind internationalization settings, i.e. comma separated cells, point decimal delimiter, and ASCII or UTF file encoding. This will ensure your file is accessible in the future. Note that Microsoft Excel by default uses comma decimal separator and semi-colon `;` separators between columns and decimal commas  if your system language is German!
- **Provide documentation**: for instance keep a README or METADATA file along with your data file to describe all column names and variables, define units and measurement procedures. 

**Never** 

- **merge cells**: That is a spreadsheet feature completely meaningless in R.  
- **rely on cell formatting** to make your file informative, e.g. colored cells or bold font. This information is lost when opening the file in R.   
- **keep empty lines**: make sure to eliminate blank space (rows or columns). (missing data values are okay!)

## header

Ideally, your raw-data file will have precisely one header row (not two rows, no empty rows!), that contains column names. 

The **column names** should  

- **be unique** within the table
- **be unambiguous** and easy to remember 
- use either only lower case *or* only upper case letters
- not contain spaces or points or special characters, except `_`.

I recommend that you use **english column names** from the start. That makes it easier to publish or share your data. 

## values

Keep in mind that there are different types of values which come with different implications for computation (and file size):

- **numerical values**: these are usually continuous float numbers, with a certain amount of decimal places (e.g. "0.00032", "13", or "240000"). They may also be discrete integer values (which is a fundamental difference when it comes to statistics and plotting).
- **factorial values**: these are character strings from a defined vocabulary of ordered or unordered factor levels, like  sometimes you use own codes (e.g. "a, b, c", or "aew, hew, sew", or treatment levels like "low", "medium", "high"). Make sure those are harmonized throughout the raw-data file (e.g. don't mix "Yes" and "yes" values!)!  A special case of factorial values are binaries (e.g. "TRUE"/"FALSE" or "Y"/"N" or "0"/"1") 
- **missing values**: there is no clear convention. Often those are labelled "NA" for true missing values, or just empty "" if the value does not apply to the data set. Some authors use clear markers which can easily be filtered, like "999".  
All this helps to keep your data files simple and unambiguous and forces you to use a clear structure. Ideally, the data file is static and not used for calculations. All calculations and transformations of your data are done in R!
- **weird values**: like date & time, geolocation, DOIs & URLs. These require particular packages and functions for handling in R. Make sure to document their meaning and format in the README file. 

Some columns may contain comments or warnings, specific explanations etc. In R these are considered as character strings (e.g. "Data missing because of bad weather conditions.") and are not exactly used for data analysis. 

# 2. read raw data 

R can read data tables of many file formats but the most basic file format are pure text files (.csv, .txt), where columns are separated by line breaks and rows are separated by a 'Separator', which is usually a comma (hence .csv = **c**omma **s**eparated **v**alue). 

We cover two-dimensional spradsheet files here, which in R are handled in 'data.frame' objects. More complex file formats like relational databases are beyond this tutorial. 

## read function

To open a file in R you will need one of the following functions.

```{r, eval = FALSE}
rawdata <- read.table("rawdata.csv") 
rawdata <- read.csv2("rawdata.csv")
rawdata <- read.csv("rawdata.csv")  
```

These are alternative functions that do mostly the same, but have different default assumptions of the data structure of your text-file "table.csv". For instance `read.csv2()` assumes `;` as field separator and comma `,` as decimal separator, as for files generated in Office with German language settings. The function parameters allow you to set the separator (`sep`) and decimal delimiter (`del`), to skip a leading line (`skip`) or to expect a header (`header`).  So, if your file does not read well into R, it's likely you have to meddle with these parameters (see `?read.csv` for further information). 

For other file formats you might require additional packages. Most notable are: 

- `readxl` package for files created with MS Excel: provides the functions `read_xls()` and `read_xlsx()`. 
- `foreign` package allows you to read data tables exported from other statistical software like SPSS, SAS, Octave (see [this comment](https://cran.r-project.org/doc/manuals/R-data.html#Importing-from-other-statistical-systems)).

## working directory

Importing data probably does not work right away. The R environment typically assumes your home directory as the working environment. Since you refer to a file outside of R, you need to tell R which place you are referring to by specifying the path to that file. There are two ways of doing this.  

1. giving an absolute path for the file
```{r, eval= FALSE}
  d1 <- read.csv("C:/Windows/Users/florian.schneider/Documents/projects/stats/rbasics/d1.csv")
```
  This can be very long, and will not work, if you switch to a different computer or share the file with colleagues. 
  
2. setting the working directory for R prior to loading the file and call a relative path for the file
```{r, eval = FALSE}
  setwd("G:/uni/projects/stats/rbasics/")
  d1 <- read.csv("data/d1.csv")
```
```{r, echo = FALSE}
  d1 <- read.csv("data/d1.csv")
```

The simplest way to manage working directories and relative paths is to use RStudio projects. For each project, RStudio will assume the working directory in the project root directory. If you open the project on a different computer or move it around on your computer, R will still find the relative paths. 

## reading multiple files at once

If you work with large bits of data, your raw data might be split across separate files or spreadsheet pages. To load them in a single call you may use a loop. In the following example, we call all files from a specific directory (called "data"):

```{r, eval = FALSE}
files <- dir("data")
filenames <- sub( ".csv", "", files)
for (i in 1:length(files)) assign(filenames[i], read.csv(files[i]))
```

## reading a file from the internet 

Instead of reading files from your local computer, you may want to refer to files published online, e.g. on Github or figshare. 
In principle, you can open any text-based file (csv or txt) that is accessible online as a download. Just enter the download link (usually what you get when right-clicking on the link and select "copy link") as source path. 

![](assets/01_onlinefiles.jpg) 

```{r}
traits <- read.table("https://datadryad.org/bitstream/handle/10255/dryad.76638/ArthropodSpeciesTraits.txt?sequence=1", 
                     sep = "\t", header = TRUE) # original publication DOI: 10.1038/sdata.2015.13 metadata: https://www.nature.com/articles/sdata201513/tables/2


```

## reading a file from BExIS

BExIS (https://www.bexis.uni-jena.de/) is the central data management platform for the Biodiversity Exploratories. You are supposed to upload your own project data here and you have access to descriptive parameters of the plots and many other project datasets. 

An R script by Dennis Heimann and Andreas Ostrowski provides the function `read_service()` (original version as BExIS ID 6080, modified by myself with interactive password prompt [from this link](https://gist.github.com/fdschneider/9f5d044d5091976741ad07dab5439a77)).

Note that you need BExIS access credentials to call this function as well as permission to download the file from the data owner (request via BExIS). The function takes a BExIS ID, your personal credentials (`user` and `pswd`) as well as the typical parameters required for `read.table()` (i.e. `sep`, `dec`).


```{r, eval = TRUE}
LUI_factors <- read.service(19266, dec = ",", user = "fdschneider")

```

A couple of shared core datasets that might be useful for your analysis: 

-  20826: Basic Information of all Experimental Plots (EPs)
-  20907: Coordinates and Inventory Overview of all Grid Plots (GPs)
-  19007: Climate data - Time Series Web Interface
-  20055: Forest EP - forest management
-  19266: Input LUI Calculation Tool (contains all LUI single factors)

By downloading files from BExIS you declare consent with the data sharing agreement of the Exploratories!

## write data

Just to complete this section, we cover writing data files that have been created in R to your disk. 
Any data.frame object can be written into a text file using `write.csv()` and `write.table()` (just analog to `read.csv()` and `read.table()`). 

```{r}
write.csv(LUI_factors, file = "LUI_factors.csv", fileEncoding = "UTF-8" )
```

Saving output after certain steps of computation can be useful to avoid repeating computation-heavy steps during your data processing. Just save the compiled data, means, or intermediate statistics in script 1 and build on top of this in script 2. 

**A note on file encoding:** There are an infinite number of standards for text file encodings, i.e. the mappings of bits and bytes to actual letters. The UTF-8 standard is the most universal and accepted encoding. If you want to publish your data on a repository, it is recommended to choose this mode. If you want to learn more about encodings, start with [this Wikipedia article](https://en.wikipedia.org/wiki/Character_encoding) and check out `?file`. 

# 3. Review content of your data object

Now you have the content of the file available in R as an object, more specifically as a 'data.frame'. You can have a look at how the data are structured using the functions `str()` or `head()` 

```{r}
str(LUI_factors)  # returns the type of data for each column of the table 
```

this shows you the overall dimensions of the table, as well as the column name, type of values and number of factor levels for each colum. You see that R recognised numerical, integer and factorial columns. We'll discuss how and if this is a correct interpretation below.

```{r} 
head(LUI_factors) 
```
This returns the first 6 rows of a dataframe by default, and is ideal for scanning the structure of the data if the tables are not too wide. 

For a more detailled summary, you may call

```{r} 
summary(LUI_factors) 
```

If applied to a data.frame, this function produces basic stats for each column, like means and quantiles of numerical values, and frequencies of factor levels. Obviously, these statistics are not always meaningful, e.g. in case of Year. 


## about data.frames

Dataframes are two-dimensional tabular structures. They can easily be accessed by column and row. Usually the columns are named by a column header, and the rows are numbered. 

Each column is a vector of the same length (= number of rows), while each vector can have different content type, i.e. numbers, factorials, characters. This is the main difference with matrices, which are homogeneous in content. In contrast, list objects may have different vector lengths for each entry. 
The most direct way to access a column in a data frame is the `$` sign:

```{r}
hist(LUI_factors$TotalGrazing)
```

We cover other ways further below. 

## fixing problems in data.frames

**A quick recap of data types in R**: There are different types of data  that R can handle. The following objects do have different specifications. 

```{r}
 x <- c(3.2323, 4.5723, 3.8356, 8.2301, 4.4285)
     str(x)
 v <- 1:6
     str(v)
 b <- c(FALSE, TRUE, FALSE, FALSE)
     str(b)
 n <- c("Agriphila_tristella", "Agriphila_tristella", "Idaea_aversata", "Agriphila_straminella", "Chiasmia_clathrata")
     str(n)
 f <- as.factor(c("A", "A", "B", "C", "A", "B", "B", "C", "C"))
     str(f)
```

This becomes very important e.g. for statistical modelling, since the types prompt different kinds of analysis or plotting. The  types also require different amount of space in the computer memory.^[Objects that contain data of mixed types will be coerced into the more flexible type. e.g. a vector `c("a", 1)` will be of type `"character"`.] 

It is useful to understand that for modelling purposes and computationally intensive tasks. 

data type  | usage                                          | bits  per value 
---------- | ---------------------------------------------- | ------------------
"double"   | for non-integer, float numbers, percentages etc. |  32
"integer"  | for count data, for factorial data             | 3 - 32
"logical"  | for boolean TRUE/FALSE values, for binary data (e.g. survival) | 1
"character" | for strings of characters, written information, non-repeating words | many thousand, 8 per character
"factor"   | categorical values, like experimental blocks, treatments, etc. ; technically not a type, but a class, but never mind (stored as integer numbers, encoded as factor levels) | 3 - 32

When reading data from a file, R 'guesses' the data type from the values it finds within a column. Often, this is not precisely what you want and you need to correct data types. 

**Advanced R:** The package `readr` provides an improved function for guessing the types of in heterogeneous data. It does a much better job interpreting logical and numerical values, or date and currency vectors. The output 

### fixing data type

LUI_factors$isVIP

### fixing column names

names(data)

### deleting columns 

data <- subset(data, select = c(-badcol))

data$badcol <- NULL

alternatively, select the columns you want (positive subsetting): 

data <- data[, c(goodcol)]

### eliminating unwanted rows


we go more into depth for selecting rows and columns in the next section of the workshop. 

### Change order of factor levels

By default, the factor levels are ordered alphabetically. E.g. 

### Fix logical vectors

For instance, logical/binary data are much easier to handle in R if they are encoded as such. If levels are encoded in the file as one out of c("T", "TRUE", "True", "true") or c("F", "FALSE", "False", "false") then R assumes they are logical. Pure integer vectors of 1 and 0 are also interpreted as TRUE and FALSE in a logical vector. In any other case, you need to translate your own encoding. 

```{r}
str(LUI_factors)

levels(LUI_factors$isVIP) <- c(FALSE, TRUE)
LUI_factors$isVIP <- as.logical(LUI_factors$isVIP) 

str(LUI_factors)
```

### harmonize factor levels

### assign NAs correctly



---

## Exercise 1

We are assuming an experiment with 12 grassland plots of varying land use intensity that are distributed in a larger landscape. The amount of fertilization, frequency of grazing by cattle and mowing are documented for each plot. Also, each plot has a small weather station that measures temperature and precipitation. On four subplots, insect diversity is assessed, two of which are fenced against grazing by large herbivores. Sampling was done in two years (2013,2014). The data files rawdata.csv and plots.csv contain the response data on the sub-plots and the explanatory data on the plots. 
 

1. read datasets  [rawdata.csv]() and  [plots.csv]() into R.  
    a) by downloading into your local working directory from ... and using `read.csv()`, or 
    b) directly from their online source.
2. revise and fix errors
    - check for missing values
    - check decimal delimiters
    - check logical values and harmonize factors

```{r, echo = FALSE}
rawdata <- read.csv("data/rawdata.csv")  
plots <- read.csv("data/plots.csv")  
```

---


# 4. Access data 

## select data columns 

Once you have read-in a data table and fixed errors, you mostly need only some of the columns to do your further calculations. Most beginners guides describe `attach()` and `detach()` to access data within a table. 

```{r, eval = FALSE}

attach(rawdata)
plot(shannon ~ fenced)
detach(rawdata)
```

This clutters the R environment with a lot of invisible objects. People do often forget to detach databases and then objects are difficult to handle and to remove. Also, if you modify those objects, they will not affect the master table, which remains in the state it was before it was attached. **Using `attach()` is considered bad practice!**

Rather you should access content directly. The easiest way would be to use the `$` selector. 

```{r, eval = FALSE}
lm(rawdata$shannon ~  rawdata$fenced)
```

or, to extract only what you need into new objects. 

```{r, eval = FALSE}
shannon <- rawdata$shannon
fenced <- rawdata$fenced
lm(shannon ~ fenced)
```

You could also use the function `with()` which creates a temporary environment where the content of your data table is available:

```{r, eval = FALSE}
with(rawdata, lm(shannon ~ fenced))
```


A very powerful function are the squared brackets  `[]` which allow you to select columns by name or by position and let you create a subset of columns.

```{r}
head(rawdata[,c("fenced","shannon")])
```


**Best practice!**: Keep the table structure of your data object intact and tell the functions were to find the columns using the `data` parameter, which is available in many statistical functions and plotting functions:

```{r, eval = FALSE}
lm(shannon ~ fenced, data = rawdata)
```

## select data rows

In complex data sets, you often want to exclude part of it before you go on with your analysis or plot only a subset based on a condition, e.g. to show the data of only one treatment and not the other.

The squared brackets `[]` can also be used to select a part of a vector or a data frame.

To exclude one row of data, e.g. because you know that the replicate was methodologically flawed or an extreme outlier, the simplest way is to do 

```{r, eval = FALSE}
data1 <- rawdata[-c(42),]
```

Or you might want to exclude a sequence of data for similar reasons. 

```{r, eval = FALSE}
data1 <- rawdata[-c(42:64),]
```


## using logical expressions
 
In most cases, the rows you want to exclude are loosely mixed into your dataset. Or you want to select rows by a condition. Conditionals are based on  'logical expressions', a concept that is common to all programming languages:

- `A == B` : A equals B 
- `A != B` : A is unequal to B
- `A < B`, `A > B` : A is less than B, A is greater than B
- `A <= B`, `A >= B` : A is less than or equal to B, A is greater than or equal to B
- `A %in% B` : The value in A is contained in vector B

Thus, if A and B are numerical values, the expression returns logical values: `TRUE` or `FALSE`. If the logical operator is comparing a value and a vector, it returns a **logical vector**!

```{r}
1:12 > 6
f
f == "A"
6 %in% 1:5 
c("SCH", "HAI", "ALB") %in% c("HAI") 
```

These logical vectors can be used to select rows from a data table. 

```{r}
# create an example data.frame 
d <- data.frame(x = f, y = c(1,1,1.4,1.2,1.3,1.2,1.4,1.2,1.1))

d
d[d$x == "C",]
d[d$y >= 1.3,]
```

An alternative method is to use the `subset()` function.  

```{r}
subset(d, x == "C")
subset(d, y >= 1.3)
```



## dropping factor levels

Any factor levels of the original vector or dataframe will be preserved if you use `[]` or `subset()`. If you want to eliminate unused factor levels you should apply the function `droplevels()`, e.g. if you want to plot figures from the data that are based on the factor levels:

```{r}
plot(d[d$x != "A", ])
plot(droplevels(d[d$x != "A", ]))

```


## summing up

You might have noticed that the squared brackets can be used for both, subsetting of rows *and* selection of columns. This is a very powerful tool, since you can use it both at the same time. This allows us to create very compact data extracts. 

```{r}
  s1 <- d1[d1$exp2 %in% c("A","B","D"), c("exp1","resp")]
  head(s1)
  s2 <- d1[d1$exp2 %in% c("C"), c("exp1","resp")]
  head(s2)
```

---

# Exercise 2 

- The rawdata object contains notes on some flawed data points. It is a good idea to eliminate them before use.^[ `data <- rawdata[rawdata$notes == ""]` ]
- The species sampling was done in 2013. Species data for 2014 are not available yet. Eliminate 2014 values from `plots`!
- merge the plot information of 2013 to the rawdata table. 
- save the result as your working data `data.Rd` or `data.csv`.

```{r}
data <- subset(rawdata, notes == "")
plots2013 <- subset(plots, year == "2013")
data <- cbind(data, plots2013[match(data$plot, plots2013$plot), -c(1,2)])
write.csv(data, "data.csv")
save(data, data, file = "data.Rd")
```

*Note*: The specific R data file format also is a simple text file, but it maintains the full object structure. This is particularly useful when saving list objects such as output of a statistical model. 


---


# 5. operations on data tables

Many basic features of R allow to transform and edit data.frames and their contents. However, with the `tidy` package, Hadley Wickham provided a powerful toolset to handle data more easily. 




## renaming values 

revalue() 
mapvalue()

## recoding categorical variables

```{r}
levels(traits$Stratum_use_short)
```

Here, `g` stands for ground-dweller, `h` for herb layer, `s` for soil-dweller, `t` for shrub & tree layer, `u` for unspecific and `w` for water-bound. For a meaningful interpretation of the vertical stratum, a re-assignment of the order might be useful. 



## mapping continuous to categorical variables

## transform() & mutate()

## get sums per row or column

## normalize

using operations of vectors

using ddply()

## summarize by group

## melt and cast


## merge data frames 

An important function when working with multiple datatables is the function `merge()`. It allows to combine two tables of different columns by matching the rows by a column in table `x` with a column in table `y`. The rows in the shorter table will be repeated to match the rows in the longer one. 

```{r}
d
p <- data.frame(id = c("A", "B", "C"), value = c(425,754,542))
p
merge(d, p, by.x = "x", by.y = "id")
```

The function `cbind()` and `rbind()` also allow to merge dataframes by appending them column-wise or row-wise. This can be used when merging data of the same structure into one table (`rbind()`) or when merging data tables of identical rows, but with different content (`cbind()`). 

```{r}
cbind(ID = 1:9, d)
rbind(d, c("C", 1.4))
rbind(d,d)
```





### EXERCISE 3

- [ ] extract all data points from `d1` that have value "A" in `exp2` into an object `s`. 
- [ ] extract all datapoints from `d2` that have response value larger than 100, and that have value "a" for `cof1`. 
- [ ] paste another column to `d2` that contains random numbers^[Hint: use function `cbind()`] 

---
